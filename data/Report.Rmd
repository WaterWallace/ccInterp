---
title: "Model Summary Report (Flextable)"
output:
  word_document
---

```{r setup, include=FALSE}

knitr::opts_chunk$set(echo = TRUE, warning = FALSE, message = FALSE)

library(dplyr)
library(tidyr)
library(broom)
library(tibble)
library(flextable)
library(ggplot2)
library(data.table)

```

## R Markdown



```{r values, include=FALSE}
#  rawvalues <- rawvalues %>% bind_rows
#  rawvalues <- melt(rawvalues, id.vars = c('actual', 'group', 'trial', 'TideRatio','d1d2Ratio', 'magnitude' ))
#  rawvalues$error <- rawvalues$value - rawvalues$actual
#  rawvalues$variable

rawvalues <- readRDS("rawvalues.rds")
pvalues <- readRDS("pvalues.rds")

```


```{r sig1, echo=FALSE, message=FALSE, results='hide'}

  # Compute percentage of TRUE values per bin and filter type
  result1 <- pvalues %>%
    group_by(TideRatioBin, variable) %>%
    summarise(
      Count_True = sum(Significant),
      Total = n(),
      Percentage = (Count_True / Total) * 100,
      .groups = 'drop'
    )

  result1 %>% na.omit %>%
  # Plot results with rotated count labels
  ggplot(aes(x = TideRatioBin, y = Percentage, fill = variable)) +
    geom_bar(stat = "identity", position = "dodge") +
    geom_text(aes(label = paste0("n=", Total)),
              position = position_dodge(width = 0.9),
              vjust = 0.5, hjust = -0.2,  # Adjust text positioning
              angle = 90, size = 4) +  # Rotate text 90 degrees
    #scale_y_continuous(labels = scales::percent_format(scale = 1)) +  # Format as percentages
    labs(title = "Percentage of Significant Results by Event/Tide Ratio", x = "Event/Tide Ratio", y = "Percent Significant (%)", fill = "Filter") +
    theme_minimal() +
    theme(axis.text.x = element_text(angle = 45, hjust = 1)) +
    ylim (c(0,100))
    coord_cartesian(clip = 'off')  # Prevent clipping of labels
    
    
```
    

    
```{r sig2, echo=FALSE, message=FALSE, results='hide'}
    

  result2 <- pvalues %>%
    group_by(d1d2Bin, variable) %>%
    summarise(
      Count_True = sum(Significant),
      Total = n(),
      Percentage = (Count_True / Total) * 100,
      .groups = 'drop'
    )



  result2 %>% na.omit %>%
      # Plot results with rotated count labels
      ggplot(aes(x = d1d2Bin, y = Percentage, fill = variable)) +
      geom_bar(stat = "identity", position = "dodge") +
      geom_text(aes(label = paste0("n=", Total)),
                position = position_dodge(width = 0.9),
                vjust = 0.5, hjust = -0.2,  # Adjust text positioning
                angle = 90, size = 4) +  # Rotate text 90 degrees
      #scale_y_continuous(labels = scales::percent_format(scale = 1)) +  # Format as percentages
      labs(title = "Percentage of Significant Results by d1d2 Ratio", x = "d1d2 Ratio", y = "Percent Significant (%)", fill = "Filter") +
      theme_minimal() +
      theme(axis.text.x = element_text(angle = 45, hjust = 1)) +
      ylim (c(0,100))
    coord_cartesian(clip = 'off')  # Prevent clipping of labels

    
```

    

    
```{r sig3, echo=FALSE, message=FALSE, results='hide'} 
    
  result3 <- pvalues %>%
    group_by(magnitudeBin, variable) %>%
    summarise(
      Count_True = sum(Significant),
      Total = n(),
      Percentage = (Count_True / Total) * 100,
      .groups = 'drop'
    )



  result3 %>% na.omit %>%
      # Plot results with rotated count labels
      ggplot(aes(x = magnitudeBin, y = Percentage, fill = variable)) +
      geom_bar(stat = "identity", position = "dodge") +
      geom_text(aes(label = paste0("n=", Total)),
                position = position_dodge(width = 0.9),
                vjust = 0.5, hjust = -0.2,  # Adjust text positioning
                angle = 90, size = 4) +  # Rotate text 90 degrees
      #scale_y_continuous(labels = scales::percent_format(scale = 1)) +  # Format as percentages
      labs(title = "Percentage of Significant Results by EventMagnitude", x = "EventMagnitude", y = "Percent Significant (%)", fill = "Filter") +
      theme_minimal() +
      theme(axis.text.x = element_text(angle = 45, hjust = 1)) +
      ylim (c(0,100))
    coord_cartesian(clip = 'off')  # Prevent clipping of labels

```
    


    
```{r stability, echo=FALSE}
    
  #pvaluessaved <- pvalues
  bwfpvalues <- pvalues[pvalues$variable == "Butterworth",]$pvalue
  ccipvalues <- pvalues[pvalues$variable == "ccInterp",]$pvalue

  pvalues %>% bind_rows %>% ggplot(aes(x = log(TideRatio), y = (pvalue), colour = variable)) +
    geom_point()

  pvalues %>% bind_rows %>% ggplot(aes(x = d1d2Ratio, y = (pvalue), colour = variable)) +
    geom_point()

  pvalues %>% bind_rows %>% ggplot(aes(x = magnitude, y = (pvalue), colour = variable)) +
    geom_point()

    
  reps <- 10000
  # from 10000 trials
  bwfpvalues[bwfpvalues < 0.05] %>% length
  paste( bwfpvalues[bwfpvalues < 0.05] %>% length /reps*100, "% Significant", sep = "")

  ccipvalues[ccipvalues < 0.05] %>% length
  paste( ccipvalues[ccipvalues < 0.05] %>% length /reps*100, "% Significant", sep = "")

  testingstability <- data.frame(cci = rep(0, reps), bwf = rep(0, reps))
  for(i in 1:reps)
  {
    testingstability$cci[i] <- ccipvalues[1:i][ccipvalues[1:i] < 0.05] %>% length /i * 100
    testingstability$bwf[i] <- bwfpvalues[1:i][bwfpvalues[1:i] < 0.05] %>% length /i * 100
  }
  tail(testingstability)

  testingstability$Index <- as.numeric(row.names(testingstability))
  melt(testingstability, id.vars = "Index") %>% ggplot(aes(x = Index, y = value, colour = variable)) +
    geom_line()

```

## Including Plots

Considering both models of the actual values and the absolute residuals provides complementary insights. Modelling the actual values helps assess how well each method predicts the signal itself, identifying biases and systematic structure in the predictions. In contrast, modelling the absolute residuals focuses on the magnitude of the errors, which speaks to the reliability and consistency of each method across varying conditions. Together, these perspectives help distinguish between methods that may appear similarly accurate on average but differ in how stable or error-prone they are under different hydrodynamic scenarios.



```{r actual, echo=FALSE, message=FALSE, results='hide'}
  #anova_model <- aov(abs(error) ~ value + group * variable * magnitude * TideRatio * d1d2Ratio, data = rawvalues)
  #summary(anova_model)

  lm_cci_interactions <- lm(actual ~ value + magnitude * log(TideRatio) * (d1d2Ratio),
                 data = rawvalues %>% dplyr::filter(group == "withInteractions" & variable == "cci"))
  lm_cci_nointeractions <- lm(actual ~ value + magnitude * log(TideRatio) * (d1d2Ratio),
                            data = rawvalues %>% dplyr::filter(group == "noInteractions" & variable == "cci"))
  lm_bwf_interactions <- lm(actual ~ value + magnitude * log(TideRatio) * (d1d2Ratio),
                            data = rawvalues %>% dplyr::filter(group == "withInteractions" & variable == "bwf"))
  lm_bwf_nointeractions <- lm(actual ~ value + magnitude * log(TideRatio) * (d1d2Ratio),
                            data = rawvalues %>% dplyr::filter(group == "noInteractions" & variable == "bwf"))

  summary(lm_cci_interactions)
  summary(lm_bwf_interactions)
  summary(lm_cci_nointeractions)
  summary(lm_bwf_nointeractions)
  
```

Here you're fitting error ~ predictors directly, so low p-values mean the predictors help explain the variation in the estimated flow values. The significance (or lack of it) here tells you:

For both CCI and BWF, nearly all terms are statistically significant in both datasets—this suggests that your model structure captures key relationships well.

However, look at log(TideRatio) for BWF:

It's not significant (p = 0.69) in the noInteractions dataset.

It becomes weakly significant (p = 0.03) when interactions are included.

This indicates that tide effects only show up in BWF when there's a complex interaction, and even then, not very strongly.

In contrast, CCI shows strong significance across the board, which suggests it's better at capturing relationships involving the tide and its interactions.



```{r actual-other-stats, echo=FALSE, message=FALSE}

# Define your model list
models <- list(
  "CCI_With" = lm_cci_interactions,
  "CCI_Without" = lm_cci_nointeractions,
  "BWF_With" = lm_bwf_interactions,
  "BWF_Without" = lm_bwf_nointeractions
)

# Extract and format p-values
pval_long <- lapply(names(models), function(name) {
  broom::tidy(models[[name]]) %>%
    select(term, p.value) %>%
    mutate(model = name)
}) %>%
  bind_rows() %>%
  mutate(
    p.value = ifelse(p.value < 0.01, "< 0.01", sprintf("%.2f", p.value))
  ) %>%
  pivot_wider(names_from = model, values_from = p.value)

# Arrange columns
pval_long <- pval_long %>%
  select(term, CCI_With, CCI_Without, BWF_With, BWF_Without)

# Rename columns for display
colnames(pval_long) <- c("Term", 
                         "cciWith Interactions", "cciNo Interactions", 
                         "bwfWith Interactions", "bwfNo Interactions")

flextable(pval_long) %>%
  add_header_row(
    values = c("", "CCI", "BWF"),
    colwidths = c(1, 2, 2)
  ) %>%
  bold(part = "header") %>%
  autofit() %>%
  width(j = 2:5, width = 1.2) %>%
  set_caption("P-Value Summary for Linear Model Terms")


```




```{r pressure, echo=FALSE, message=FALSE}
# Extract model metrics
get_model_metrics <- function(model) {
  r2 <- summary(model)$r.squared
  rmse <- sqrt(mean(residuals(model)^2))
  return(c(R2 = r2, RMSE = rmse))
}

metrics <- lapply(models, get_model_metrics) %>%
  do.call(cbind, .) %>%
  as.data.frame() %>%
  rownames_to_column("Metric") %>%
  mutate(across(-Metric, ~sprintf("%.2f", .))) %>%
  select(Metric, CCI_With, CCI_Without, BWF_With, BWF_Without)

# Rename columns
colnames(metrics) <- c("Metric", 
                         "cciWith Interactions", "cciNo Interactions", 
                         "bwfWith Interactions", "bwfNo Interactions")

# Generate flextable
flextable(metrics) %>%
  add_header_row(
    values = c("", "CCI", "BWF"),
    colwidths = c(1, 2, 2)
  ) %>%
  bold(part = "header") %>%
  autofit() %>%
  width(j = 2:5, width = 1.2) %>%
  set_caption("Model Performance Metrics (R² and RMSE)")

```

This is especially useful as a proxy for heteroscedasticity (non-constant variance). Here, a significant term suggests that the magnitude of errors varies depending on the value of that predictor. So:

In CCI, notice how log(TideRatio) and d1d2Ratio lose significance in the withInteractions dataset.

This suggests that once the model accounts for tide/flood interactions, those variables no longer drive error variance—they’ve been successfully “explained away”.

That’s evidence of a better fit: the model captures the structure of variability.

In BWF, the p-values remain low across both datasets, including for interaction terms.

This implies that residuals still vary systematically with those variables, even after fitting the model.

This may mean that BWF isn’t adequately capturing the nonlinearities introduced by tide-flood interactions—it’s leaving structure in the residuals.

```{r actual2, echo=FALSE, message=FALSE, results='hide'}
  #anova_model <- aov(abs(error) ~ value + group * variable * magnitude * TideRatio * d1d2Ratio, data = rawvalues)
  #summary(anova_model)

  lm_cci_interactions <- lm(abs(error) ~ magnitude * log(TideRatio) * (d1d2Ratio),
                 data = rawvalues %>% dplyr::filter(group == "withInteractions" & variable == "cci"))
  lm_cci_nointeractions <- lm(abs(error) ~ magnitude * log(TideRatio) * (d1d2Ratio),
                            data = rawvalues %>% dplyr::filter(group == "noInteractions" & variable == "cci"))
  lm_bwf_interactions <- lm(abs(error) ~  magnitude * log(TideRatio) * (d1d2Ratio),
                            data = rawvalues %>% dplyr::filter(group == "withInteractions" & variable == "bwf"))
  lm_bwf_nointeractions <- lm(abs(error) ~  magnitude * log(TideRatio) * (d1d2Ratio),
                            data = rawvalues %>% dplyr::filter(group == "noInteractions" & variable == "bwf"))

  summary(lm_cci_interactions)
  summary(lm_bwf_interactions)
  summary(lm_cci_nointeractions)
  summary(lm_bwf_nointeractions)
  
  
```



```{r actual-other-stats2, echo=FALSE, message=FALSE}

# Define your model list
models <- list(
  "CCI_With" = lm_cci_interactions,
  "CCI_Without" = lm_cci_nointeractions,
  "BWF_With" = lm_bwf_interactions,
  "BWF_Without" = lm_bwf_nointeractions
)

# Extract and format p-values
pval_long <- lapply(names(models), function(name) {
  broom::tidy(models[[name]]) %>%
    select(term, p.value) %>%
    mutate(model = name)
}) %>%
  bind_rows() %>%
  mutate(
    p.value = ifelse(p.value < 0.01, "< 0.01", sprintf("%.2f", p.value))
  ) %>%
  pivot_wider(names_from = model, values_from = p.value)

# Arrange columns
pval_long <- pval_long %>%
  select(term, CCI_With, CCI_Without, BWF_With, BWF_Without)

# Rename columns for display
colnames(pval_long) <- c("Term", 
                         "cciWith Interactions", "cciNo Interactions", 
                         "bwfWith Interactions", "bwfNo Interactions")

library(flextable)
# Generate flextable
flextable(pval_long) %>%
  add_header_row(
    values = c("", "CCI", "BWF"),
    colwidths = c(1, 2, 2)
  ) %>%
  bold(part = "header") %>%
  autofit() %>%
  width(j = 2:5, width = 1.2) %>%
  set_caption("P-Value Summary for Linear Model Terms")



```



```{r pressure2, echo=FALSE, message=FALSE}
# Extract model metrics
get_model_metrics <- function(model) {
  r2 <- summary(model)$r.squared
  rmse <- sqrt(mean(residuals(model)^2))
  return(c(R2 = r2, RMSE = rmse))
}

metrics <- lapply(models, get_model_metrics) %>%
  do.call(cbind, .) %>%
  as.data.frame() %>%
  rownames_to_column("Metric") %>%
  mutate(across(-Metric, ~sprintf("%.2f", .))) %>%
  select(Metric, CCI_With, CCI_Without, BWF_With, BWF_Without)

# Rename columns
colnames(metrics) <- c("Metric", 
                         "cciWith Interactions", "cciNo Interactions", 
                         "bwfWith Interactions", "bwfNo Interactions")

# Generate flextable
flextable(metrics) %>%
  add_header_row(
    values = c("", "CCI", "BWF"),
    colwidths = c(1, 2, 2)
  ) %>%
  bold(part = "header") %>%
  autofit() %>%
  width(j = 2:5, width = 1.2) %>%
  set_caption("Model Performance Metrics (R² and RMSE)")

```


Low p-values for interaction terms in the residual models suggest that those interactions are not being fully accounted for—they still explain some of the error.

In CCI, those terms often become non-significant once interactions are modeled → this implies better model adequacy.

In BWF, persistent low p-values across terms (even in the residuals) suggest the method doesn’t fully explain the structure introduced by tidal interactions.
